{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu8XzXo9II_v"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0. SETUP BÀSIC – PER EXECUTAR A GOOGLE COLAB (VERSIÓ 3 - ROBUSTA)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q sentence-transformers lightgbm\n",
        "\n",
        "import os\n",
        "import ast\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DATA_DIR = \"/content/\"\n",
        "train_path = os.path.join(DATA_DIR, \"train.csv\")\n",
        "test_path  = os.path.join(DATA_DIR, \"test.csv\")\n",
        "\n",
        "print(\"Train path:\", train_path)\n",
        "print(\"Test path:\", test_path)\n",
        "\n",
        "try:\n",
        "    # Usem sep=';', forcem header=0 i mantenim 'skip' per les línies de dades dolentes\n",
        "    train = pd.read_csv(train_path, sep=';', header=0, on_bad_lines='skip')\n",
        "    test  = pd.read_csv(test_path,  sep=';', header=0, on_bad_lines='skip')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error crític durant la càrrega: {e}\")\n",
        "    print(\"Provant amb 'python' engine (més lent però més tolerant)...\")\n",
        "    train = pd.read_csv(train_path, sep=';', header=0, engine=\"python\", on_bad_lines=\"skip\")\n",
        "    test  = pd.read_csv(test_path,  sep=';', header=0, engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "\n",
        "# Neteja de columnes fantasma (les ;;;; del final)\n",
        "train.dropna(axis=1, how='all', inplace=True)\n",
        "test.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# Normalitzem un typo que he vist:\n",
        "if 'has_plus_sizes' in test.columns:\n",
        "    test.rename(columns={'has_plus_sizes': 'has_plus_size'}, inplace=True)\n",
        "\n",
        "print(\"Train shape (raw):\", train.shape)\n",
        "print(\"Test shape (raw):\", test.shape)\n",
        "\n",
        "# --- VERIFICACIÓ DE COLUMNES CLAU ---\n",
        "required_cols_train = ['ID', 'phase_in', 'weekly_demand', 'year']\n",
        "required_cols_test = ['ID', 'phase_in']\n",
        "\n",
        "# Comprovem Train\n",
        "missing_in_train = [col for col in required_cols_train if col not in train.columns]\n",
        "if 'phase_in' not in train.columns:\n",
        "    print(\"ERROR GREU: 'phase_in' no s'ha carregat a 'train'. Columnes presents:\")\n",
        "    print(train.columns.tolist())\n",
        "    raise KeyError(\"La columna 'phase_in' falta a train.csv després de la càrrega.\")\n",
        "\n",
        "# Comprovem Test\n",
        "if 'phase_in' not in test.columns:\n",
        "    print(\"ERROR GREU: 'phase_in' no s'ha carregat a 'test'. Columnes presents:\")\n",
        "    print(test.columns.tolist())\n",
        "    raise KeyError(\"La columna 'phase_in' falta a test.csv després de la càrrega.\")\n",
        "\n",
        "print(\"Verificació de columnes (phase_in) OK.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. FUNCCIONS AUXILIARS\n",
        "# ============================================================\n",
        "\n",
        "def parse_image_embedding(col):\n",
        "    \"\"\"\n",
        "    Assumeix que image_embedding és una string tipus \"[0.1, 0.2, ...]\".\n",
        "    Si no és el cas, adapta aquesta funció.\n",
        "    \"\"\"\n",
        "    def _parse(x):\n",
        "        if isinstance(x, str):\n",
        "            try:\n",
        "                return np.array(ast.literal_eval(x), dtype=\"float32\")\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "        elif isinstance(x, (list, np.ndarray)):\n",
        "            return np.array(x, dtype=\"float32\")\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    emb_list = col.apply(_parse).values\n",
        "    # Filtrar NaNs\n",
        "    valid_emb = [e for e in emb_list if isinstance(e, np.ndarray)]\n",
        "    dim = valid_emb[0].shape[0]\n",
        "    # Omplir NaNs amb zeros\n",
        "    fixed = []\n",
        "    for e in emb_list:\n",
        "        if not isinstance(e, np.ndarray):\n",
        "            fixed.append(np.zeros(dim, dtype=\"float32\"))\n",
        "        else:\n",
        "            fixed.append(e)\n",
        "    return np.vstack(fixed)\n",
        "\n",
        "\n",
        "def build_text_description(row):\n",
        "    \"\"\"\n",
        "    Crea una descripció textual del producte a partir d'atributs rellevants (VERSIÓ CORREGIDA).\n",
        "    \"\"\"\n",
        "    parts = [\n",
        "        str(row.get(\"aggregated_family\", \"\")),\n",
        "        str(row.get(\"family\", \"\")),\n",
        "        str(row.get(\"category\", \"\")),\n",
        "        str(row.get(\"silhouette_type\", \"\")),\n",
        "        str(row.get(\"length_type\", \"\")),\n",
        "        str(row.get(\"waist_type\", \"\")),\n",
        "        str(row.get(\"sleeve_length_type\", \"\")),\n",
        "        str(row.get(\"neck_lapel_type\", \"\")), # Afegida\n",
        "        str(row.get(\"fabric\", \"\")),\n",
        "        str(row.get(\"print_type\", \"\")),\n",
        "        str(row.get(\"color_name\", \"\")),\n",
        "        str(row.get(\"moment\", \"\")),\n",
        "        # str(row.get(\"ocassion\", \"\")), # Eliminada\n",
        "        str(row.get(\"archetype\", \"\")),\n",
        "    ]\n",
        "    parts = [p for p in parts if p and p.lower() != \"nan\"]\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "def add_year_if_missing(df):\n",
        "    \"\"\"\n",
        "    Si no hi ha 'year' però sí 'id_season', pots intentar extreure l'any.\n",
        "    Si ja tens 'year', no fa res.\n",
        "    \"\"\"\n",
        "    if \"year\" not in df.columns:\n",
        "        if \"id_season\" in df.columns:\n",
        "            # EXEMPLE: si id_season porta l'any dins o és un codi que permet obtenir-lo.\n",
        "            # Aquí deixem un placeholder. Adapta a la realitat.\n",
        "            # Ara mateix no fa res intel·ligent:\n",
        "            df[\"year\"] = df[\"id_season\"]\n",
        "        else:\n",
        "            raise ValueError(\"No 'year' nor 'id_season' in dataframe.\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "6Hsq9BB7JOoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. CREAR 'year' DES DE 'phase_in' I PREPARAR TARGET D_TOTAL\n",
        "# ============================================================\n",
        "\n",
        "def create_year_from_phase_in(df):\n",
        "    # Convertim phase_in a string i extreiem els 4 dígits de l'any\n",
        "    # Assegurem que és string abans d'aplicar .str\n",
        "    df['phase_in_str'] = df['phase_in'].astype(str)\n",
        "\n",
        "    # Extreiem l'any (YYYY) de formats com DD/MM/YYYY\n",
        "    df['year'] = df['phase_in_str'].str.extract(r'(\\d{4})')\n",
        "\n",
        "    # Convertim a numèric i omplim si hi ha algun error\n",
        "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "\n",
        "    # Si algun falla, omplim amb la moda (l'any més comú)\n",
        "    if df['year'].isnull().any():\n",
        "        mode_year = df['year'].mode()[0]\n",
        "        df['year'] = df['year'].fillna(mode_year)\n",
        "\n",
        "    df['year'] = df['year'].astype(int)\n",
        "    df = df.drop(columns=['phase_in_str'])\n",
        "    return df\n",
        "\n",
        "# Apliquem la nova funció\n",
        "train = create_year_from_phase_in(train)\n",
        "test  = create_year_from_phase_in(test)\n",
        "\n",
        "print(f\"Anys detectats a Train: {train['year'].unique()}\")\n",
        "print(f\"Anys detectats a Test: {test['year'].unique()}\")\n",
        "\n",
        "\n",
        "# Assegurem tipus numèric on toca\n",
        "train[\"weekly_demand\"] = pd.to_numeric(train[\"weekly_demand\"], errors=\"coerce\").fillna(0)\n",
        "train[\"weekly_sales\"]  = pd.to_numeric(train[\"weekly_sales\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Agreguem a nivell ID (target D_total i info bàsica)\n",
        "agg_train = (\n",
        "    train\n",
        "    .groupby(\"ID\")\n",
        "    .agg(\n",
        "        D_total=(\"weekly_demand\", \"sum\"),   # TARGET\n",
        "        sales_total=(\"weekly_sales\", \"sum\"),\n",
        "        first_week=(\"num_week_iso\", \"min\"),\n",
        "        last_week=(\"num_week_iso\", \"max\")\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "agg_train[\"life_cycle_real\"] = agg_train[\"last_week\"] - agg_train[\"first_week\"] + 1\n",
        "\n",
        "print(\"Unique IDs in train:\", agg_train.shape[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pXRXkE01JgKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. EXTRACCIÓ D’ATRIBUTS ESTÀTICS (UNA FILA PER ID) - CORREGIT\n",
        "# ============================================================\n",
        "\n",
        "static_cols = [\n",
        "    \"ID\", \"id_season\", \"aggregated_family\", \"family\", \"category\", \"fabric\",\n",
        "    \"color_name\", \"color_rgb\", \"length_type\", \"silhouette_type\", \"waist_type\",\n",
        "    \"sleeve_length_type\", \"heel_shape_type\", \"toecap_type\", \"woven_structure\",\n",
        "    \"knit_structure\", \"print_type\", \"archetype\", \"moment\", \"ocassion\",\n",
        "    \"phase_in\", \"phase_out\", \"life_cycle_length\",\n",
        "    \"num_stores\", \"num_sizes\", \"has_plus_size\", \"price\", \"year\",\n",
        "    \"image_embedding\"\n",
        "]\n",
        "\n",
        "# No totes les columnes poden existir en tots els datasets; filtrem\n",
        "static_cols_train = [c for c in static_cols if c in train.columns]\n",
        "static_cols_test = [c for c in static_cols if c in test.columns]\n",
        "\n",
        "# --- LÒGICA PER STATIC_TRAIN (Té dades setmanals) ---\n",
        "# Treiem 'ID' de la llista de columnes a seleccionar, perquè ja és l'índex del groupby\n",
        "cols_to_agg_train = [c for c in static_cols_train if c != \"ID\"]\n",
        "\n",
        "static_train = (\n",
        "    train\n",
        "    .sort_values([\"ID\", \"num_week_iso\"]) # <-- 'train' sí que té 'num_week_iso'\n",
        "    .groupby(\"ID\")[cols_to_agg_train]\n",
        "    .first()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# --- NOVA LÒGICA PER STATIC_TEST (Ja és 1 fila/ID i no té 'num_week_iso') ---\n",
        "# El 'test' ja ve agregat (1 fila per ID), no cal fer groupby ni sort.\n",
        "# Només hem de seleccionar les columnes estàtiques que existeixin.\n",
        "static_test = test[static_cols_test].copy()\n",
        "# --- FI DEL CANVI ---\n",
        "\n",
        "\n",
        "print(\"static_train shape:\", static_train.shape)\n",
        "print(\"static_test shape:\", static_test.shape)\n",
        "\n",
        "# Merge de target + estàtics per train\n",
        "train_id = agg_train.merge(static_train, on=\"ID\", how=\"left\")"
      ],
      "metadata": {
        "id": "2vcnhqtoJjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. EMBEDDING DE TEXT\n",
        "# ============================================================\n",
        "\n",
        "train_id[\"text_desc\"] = train_id.apply(build_text_description, axis=1)\n",
        "static_test[\"text_desc\"] = static_test.apply(build_text_description, axis=1)\n",
        "\n",
        "print(\"Exemple descripció:\", train_id[\"text_desc\"].iloc[0])\n",
        "\n",
        "text_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "train_text_emb = text_model.encode(\n",
        "    train_id[\"text_desc\"].tolist(),\n",
        "    batch_size=128,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "test_text_emb = text_model.encode(\n",
        "    static_test[\"text_desc\"].tolist(),\n",
        "    batch_size=128,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "# Reducció dimensional\n",
        "pca_text = PCA(n_components=50, random_state=42)\n",
        "train_text_pca = pca_text.fit_transform(train_text_emb)\n",
        "test_text_pca = pca_text.transform(test_text_emb)\n",
        "\n",
        "text_cols = [f\"text_emb_{i}\" for i in range(train_text_pca.shape[1])]\n",
        "df_train_text = pd.DataFrame(train_text_pca, columns=text_cols, index=train_id.index)\n",
        "df_test_text  = pd.DataFrame(test_text_pca,  columns=text_cols, index=static_test.index)\n"
      ],
      "metadata": {
        "id": "jbTUnJB6JkxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. EMBEDDING D’IMATGE\n",
        "# ============================================================\n",
        "\n",
        "if \"image_embedding\" not in train_id.columns:\n",
        "    raise ValueError(\"No 'image_embedding' column in train_id; adapta aquesta part.\")\n",
        "\n",
        "img_emb_train = parse_image_embedding(train_id[\"image_embedding\"])\n",
        "img_emb_test  = parse_image_embedding(static_test[\"image_embedding\"])\n",
        "\n",
        "print(\"Image embedding shape (train):\", img_emb_train.shape)\n",
        "print(\"Image embedding shape (test):\", img_emb_test.shape)\n",
        "\n",
        "# PCA per comprimir\n",
        "pca_img = PCA(n_components=50, random_state=42)\n",
        "train_img_pca = pca_img.fit_transform(img_emb_train)\n",
        "test_img_pca  = pca_img.transform(img_emb_test)\n",
        "\n",
        "img_cols = [f\"img_emb_{i}\" for i in range(train_img_pca.shape[1])]\n",
        "df_train_img = pd.DataFrame(train_img_pca, columns=img_cols, index=train_id.index)\n",
        "df_test_img  = pd.DataFrame(test_img_pca,  columns=img_cols, index=static_test.index)\n"
      ],
      "metadata": {
        "id": "1UYeYM49JmQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. FEATURES TEMPORALS AVANÇADES\n",
        "#   (category, family, silhouette_type, moment) - CORREGIDA\n",
        "# ============================================================\n",
        "\n",
        "# Ens assegurem que tenim aquestes columnes al raw train\n",
        "# --- 'ocassion' eliminada de la llista ---\n",
        "for col in [\"category\", \"family\", \"silhouette_type\", \"moment\"]:\n",
        "    if col not in train.columns:\n",
        "        train[col] = \"unknown\" # Afegim 'unknown' si no existeix\n",
        "\n",
        "# Helper: calcula agregats per (group_col, year) i deriva features de tendència\n",
        "def build_temporal_features(df, group_col, demand_col=\"weekly_demand\", price_col=\"price\"):\n",
        "    \"\"\"\n",
        "    df: dataframe setmanal (train)\n",
        "    group_col: columna (str) de agrupació, ex 'category', 'family', etc.\n",
        "    Retorna un dataframe amb:\n",
        "      group_col, year + features:\n",
        "        total_demand, mean_price, num_products,\n",
        "        demand_growth, demand_trend_slope, demand_trend_vol, ...\n",
        "    \"\"\"\n",
        "    tmp = (\n",
        "        df\n",
        "        .groupby([group_col, \"year\"])\n",
        "        .agg(\n",
        "            total_demand=(demand_col, \"sum\"),\n",
        "            mean_price=(price_col, \"mean\"),\n",
        "            num_products=(\"ID\", \"nunique\")\n",
        "        )\n",
        "        .reset_index()\n",
        "        .sort_values([group_col, \"year\"])\n",
        "    )\n",
        "\n",
        "    # Creixement interanual\n",
        "    tmp[\"total_demand_prev\"] = tmp.groupby(group_col)[\"total_demand\"].shift(1)\n",
        "    tmp[\"demand_growth\"] = tmp[\"total_demand\"] / tmp[\"total_demand_prev\"] - 1\n",
        "    tmp[\"demand_growth\"] = tmp[\"demand_growth\"].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    # Ara calculem slope de la demanda per grup (usant els punts any vs total_demand)\n",
        "    slopes = []\n",
        "    vols   = []\n",
        "\n",
        "    for g, sub in tmp.groupby(group_col):\n",
        "        years = sub[\"year\"].values\n",
        "        dem   = sub[\"total_demand\"].values\n",
        "\n",
        "        if len(sub) >= 2:\n",
        "            # slope via regressió lineal\n",
        "            x = years.astype(float)\n",
        "            y = dem.astype(float)\n",
        "            # normalitzem x per estabilitat\n",
        "            x_norm = x - x.mean()\n",
        "            slope = np.polyfit(x_norm, y, 1)[0]\n",
        "            vol = np.std(sub[\"demand_growth\"].values)\n",
        "        else:\n",
        "            slope = 0.0\n",
        "            vol = 0.0\n",
        "\n",
        "        slopes.extend([slope] * len(sub))\n",
        "        vols.extend([vol] * len(sub))\n",
        "\n",
        "    tmp[\"demand_trend_slope\"] = slopes\n",
        "    tmp[\"demand_trend_vol\"]   = vols\n",
        "\n",
        "    # Renombrem columnes per incloure el nom del grup\n",
        "    out = tmp.rename(columns={\n",
        "        \"total_demand\": f\"{group_col}_total_demand\",\n",
        "        \"mean_price\":   f\"{group_col}_mean_price\",\n",
        "        \"num_products\": f\"{group_col}_num_products\",\n",
        "        \"demand_growth\": f\"{group_col}_demand_growth\",\n",
        "        \"demand_trend_slope\": f\"{group_col}_demand_trend_slope\",\n",
        "        \"demand_trend_vol\": f\"{group_col}_demand_trend_vol\"\n",
        "    })\n",
        "\n",
        "    # --- CANVI CLAU AQUÍ ---\n",
        "    # Esborrem la columna intermèdia 'total_demand_prev' abans de retornar\n",
        "    if \"total_demand_prev\" in out.columns:\n",
        "        out = out.drop(columns=[\"total_demand_prev\"])\n",
        "    # --- FI DEL CANVI ---\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Construïm features temporals per cada dimensió\n",
        "temp_cat  = build_temporal_features(train, \"category\")\n",
        "temp_fam  = build_temporal_features(train, \"family\")\n",
        "temp_sil  = build_temporal_features(train, \"silhouette_type\")\n",
        "temp_mom  = build_temporal_features(train, \"moment\")\n",
        "# temp_oca  = build_temporal_features(train, \"ocassion\") # <-- Línia eliminada\n",
        "\n",
        "# Merge a train_id i static_test\n",
        "def merge_temporal_features(base_df, temp_df, group_col):\n",
        "    cols_merge = [group_col, \"year\"]\n",
        "\n",
        "    # Assegurem que les columnes de merge existeixen al base_df\n",
        "    if group_col not in base_df.columns:\n",
        "        base_df[group_col] = \"unknown\"\n",
        "\n",
        "    feat_cols = [c for c in temp_df.columns if c not in cols_merge]\n",
        "    return base_df.merge(temp_df[cols_merge + feat_cols],\n",
        "                         on=cols_merge,\n",
        "                         how=\"left\")\n",
        "\n",
        "train_id = merge_temporal_features(train_id, temp_cat, \"category\")\n",
        "train_id = merge_temporal_features(train_id, temp_fam, \"family\")\n",
        "train_id = merge_temporal_features(train_id, temp_sil, \"silhouette_type\")\n",
        "train_id = merge_temporal_features(train_id, temp_mom, \"moment\")\n",
        "# train_id = merge_temporal_features(train_id, temp_oca, \"ocassion\") # <-- Línia eliminada\n",
        "\n",
        "static_test = merge_temporal_features(static_test, temp_cat, \"category\")\n",
        "static_test = merge_temporal_features(static_test, temp_fam, \"family\")\n",
        "static_test = merge_temporal_features(static_test, temp_sil, \"silhouette_type\")\n",
        "static_test = merge_temporal_features(static_test, temp_mom, \"moment\")\n",
        "# static_test = merge_temporal_features(static_test, temp_oca, \"ocassion\") # <-- Línia eliminada\n",
        "\n",
        "# Omplim NaNs temporals amb valors neutres (LÒGICA CORREGIDA)\n",
        "temporal_cols_train = [c for c in train_id.columns if \"_demand_\" in c or \"_mean_price\" in c or \"_num_products\" in c]\n",
        "train_id[temporal_cols_train] = train_id[temporal_cols_train].fillna(0)\n",
        "\n",
        "temporal_cols_test = [c for c in static_test.columns if \"_demand_\" in c or \"_mean_price\" in c or \"_num_products\" in c]\n",
        "static_test[temporal_cols_test] = static_test[temporal_cols_test].fillna(0)\n",
        "# --- FI DE LA CORRECCIÓ ---\n",
        "\n",
        "print(\"Train_id shape after temporal merge:\", train_id.shape)\n",
        "print(\"Static_test shape after temporal merge:\", static_test.shape)"
      ],
      "metadata": {
        "id": "KoIJSsxPJnuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. CONSTRUCCIÓ DATASET FINAL (FEATURES COMPLETES) - CORREGIT\n",
        "# ============================================================\n",
        "\n",
        "# Afegim embeddings\n",
        "train_feat = pd.concat([train_id, df_train_text, df_train_img], axis=1)\n",
        "test_feat  = pd.concat([static_test, df_test_text, df_test_img], axis=1)\n",
        "\n",
        "# --- NOU CANVI: Conversió manual de 'has_plus_size' ---\n",
        "# Convertim la columna 'has_plus_size' (que és string \"true\"/\"false\") a integer (1/0)\n",
        "# Ho fem abans de 'get_dummies' per evitar que entri com a 'object' a LightGBM\n",
        "\n",
        "map_dict = {'true': 1, 'false': 0}\n",
        "\n",
        "if \"has_plus_size\" in train_feat.columns:\n",
        "    # Usem .map per convertir. Els valors no trobats (NaNs) es queden com NaN\n",
        "    train_feat['has_plus_size'] = train_feat['has_plus_size'].map(map_dict)\n",
        "    # Omplim els NaNs que hagin quedat (o que ja hi eren) amb 0 (valor neutre)\n",
        "    train_feat['has_plus_size'] = train_feat['has_plus_size'].fillna(0).astype(int)\n",
        "\n",
        "if \"has_plus_size\" in test_feat.columns:\n",
        "    test_feat['has_plus_size'] = test_feat['has_plus_size'].map(map_dict)\n",
        "    test_feat['has_plus_size'] = test_feat['has_plus_size'].fillna(0).astype(int)\n",
        "\n",
        "# --- FI DEL CANVI ---\n",
        "\n",
        "\n",
        "# Guardem target i IDs\n",
        "y = train_feat[\"D_total\"].values\n",
        "train_ids = train_feat[\"ID\"].values\n",
        "test_ids  = test_feat[\"ID\"].values\n",
        "\n",
        "# Treure columnes que no volem com a input\n",
        "drop_cols = [\n",
        "    \"D_total\", \"sales_total\", \"first_week\", \"last_week\", \"life_cycle_real\",\n",
        "    \"text_desc\", \"image_embedding\",\n",
        "    \"weekly_sales\", \"weekly_demand\", \"Production\"\n",
        "]\n",
        "\n",
        "for c in drop_cols:\n",
        "    if c in train_feat.columns:\n",
        "        train_feat = train_feat.drop(columns=[c], errors='ignore') # Afegit errors='ignore'\n",
        "    if c in test_feat.columns:\n",
        "        test_feat = test_feat.drop(columns=[c], errors='ignore') # Afegit errors='ignore'\n",
        "\n",
        "# ID no entra com a feature\n",
        "if \"ID\" in train_feat.columns:\n",
        "    train_feat = train_feat.drop(columns=[\"ID\"])\n",
        "if \"ID\" in test_feat.columns:\n",
        "    test_feat = test_feat.drop(columns=[\"ID\"])\n",
        "\n",
        "# Separem tipus objecte per fer dummies\n",
        "cat_cols = train_feat.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "full = pd.concat([train_feat, test_feat], axis=0)\n",
        "\n",
        "# Dummies ràpids\n",
        "full = pd.get_dummies(full, columns=cat_cols, dummy_na=True)\n",
        "\n",
        "train_X = full.iloc[:len(train_feat)].reset_index(drop=True)\n",
        "test_X  = full.iloc[len(train_feat):].reset_index(drop=True)\n",
        "\n",
        "# --- NOU CANVI: Netejar els noms de les columnes per LightGBM ---\n",
        "# Reemplacem qualsevol caràcter que no sigui lletra, número o _ per un _\n",
        "import re\n",
        "train_X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in train_X.columns]\n",
        "test_X.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in test_X.columns]\n",
        "# --- FI DEL CANVI ---\n",
        "\n",
        "print(\"train_X shape:\", train_X.shape)\n",
        "print(\"test_X shape:\", train_X.shape)"
      ],
      "metadata": {
        "id": "BSVZ3u45Jpni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. SPLIT TEMPORAL TRAIN / VALID\n",
        "# ============================================================\n",
        "\n",
        "# Recuperem 'year' per fer split temporal (abans de treure-la de features)\n",
        "# Ho traiem de train_id (mateix ordre que y)\n",
        "train_years = train_id[\"year\"].values\n",
        "\n",
        "last_year = np.max(train_years)\n",
        "print(\"Últim any en train:\", last_year)\n",
        "\n",
        "is_valid = train_years == last_year\n",
        "\n",
        "X_train = train_X[~is_valid]\n",
        "X_valid = train_X[is_valid]\n",
        "y_train = y[~is_valid]\n",
        "y_valid = y[is_valid]\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Valid size:\", X_valid.shape)\n"
      ],
      "metadata": {
        "id": "PMikePYeKCRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9. LIGHTGBM QUANTÍLIC – ENTRENAR DIVERSOS QUANTILS (CORREGIT)\n",
        "# ============================================================\n",
        "\n",
        "def train_lgb_quantile(X_tr, y_tr, X_va, y_va, alpha):\n",
        "    params = {\n",
        "        \"objective\": \"quantile\",\n",
        "        \"alpha\": alpha,\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"num_leaves\": 63,\n",
        "        \"min_data_in_leaf\": 80,\n",
        "        \"feature_fraction\": 0.8,\n",
        "        \"bagging_fraction\": 0.8,\n",
        "        \"bagging_freq\": 1,\n",
        "        \"metric\": \"quantile\",\n",
        "        \"verbose\": -1,\n",
        "        \"seed\": 42,\n",
        "    }\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
        "    lgb_valid = lgb.Dataset(X_va, label=y_va)\n",
        "\n",
        "    # --- CANVI CLAU AQUÍ ---\n",
        "    # 'early_stopping_rounds' i 'verbose_eval' ara van dins de 'callbacks'\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_train,\n",
        "        valid_sets=[lgb_train, lgb_valid],\n",
        "        num_boost_round=3000,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=150, verbose=False), # Atura si no millora en 150 rondes\n",
        "            lgb.log_evaluation(period=200) # Mostra el log cada 200 rondes\n",
        "        ]\n",
        "    )\n",
        "    # --- FI DEL CANVI ---\n",
        "\n",
        "    return model\n",
        "\n",
        "quantiles = [0.5, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
        "models = {}\n",
        "pred_valid = {}\n",
        "\n",
        "for q in quantiles:\n",
        "    print(f\"\\n========== Entrenant model per quantil {q} ==========\")\n",
        "    m = train_lgb_quantile(X_train, y_train, X_valid, y_valid, alpha=q)\n",
        "    models[q] = m\n",
        "\n",
        "    # Assegurem que 'best_iteration' té un valor\n",
        "    best_iter = m.best_iteration if m.best_iteration is not None else 3000\n",
        "    pred_valid[q] = m.predict(X_valid, num_iteration=best_iter)"
      ],
      "metadata": {
        "id": "OQyzakHyKE-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 10. CÀLCUL DEL VAR PER CADA QUANTIL I SELECCIÓ DEL MILLOR\n",
        "# ============================================================\n",
        "\n",
        "valid_demand_real = y_valid  # D_total real a validació\n",
        "\n",
        "var_scores = {}\n",
        "\n",
        "for q in quantiles:\n",
        "    prod_q = np.maximum(pred_valid[q], 1)  # evitar 0\n",
        "    full_price_sales_q = np.minimum(valid_demand_real, prod_q)\n",
        "    var_q = (full_price_sales_q / prod_q).mean()\n",
        "    var_scores[q] = var_q\n",
        "\n",
        "print(\"VAR per quantil:\")\n",
        "for q, v in var_scores.items():\n",
        "    print(f\"  Quantil {q}: VAR = {v:.4f}\")\n",
        "\n",
        "best_q = max(var_scores, key=var_scores.get)\n",
        "print(\"\\n>>> MILLOR QUANTIL:\", best_q, \"amb VAR:\", var_scores[best_q])\n"
      ],
      "metadata": {
        "id": "lOS-MwiqKGMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 11. PREDICCIÓ EN TEST I SUBMISSION\n",
        "# ============================================================\n",
        "\n",
        "best_model = models[best_q]\n",
        "\n",
        "test_pred = best_model.predict(test_X, num_iteration=best_model.best_iteration)\n",
        "test_prod = np.maximum(test_pred, 1).astype(int)  # producció mínima 1 unitat\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_ids,\n",
        "    \"Production\": test_prod\n",
        "})\n",
        "\n",
        "out_path = os.path.join(DATA_DIR, \"submission.csv\")\n",
        "submission.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"Submission guardada a:\", out_path)\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "0yW-tlRuKHpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}