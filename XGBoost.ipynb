{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si cal:\n",
    "#pip install pandas numpy scikit-learn lightgbm sentence-transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Agreguem demanda potencial per ID\n",
    "agg_train = (\n",
    "    train\n",
    "    .groupby(\"ID\")\n",
    "    .agg(\n",
    "        D_total=(\"weekly_demand\", \"sum\"),   # TARGET\n",
    "        sales_total=(\"weekly_sales\", \"sum\"),\n",
    "        first_week=(\"num_week_iso\", \"min\"),\n",
    "        last_week=(\"num_week_iso\", \"max\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_train[\"life_cycle_real\"] = agg_train[\"last_week\"] - agg_train[\"first_week\"] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a40483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agafem el primer registre per ID com a \"representant\" dels atributs estàtics\n",
    "static_cols = [\n",
    "    \"ID\", \"id_season\", \"aggregated_family\", \"family\", \"category\", \"fabric\",\n",
    "    \"color_name\", \"color_rgb\", \"length_type\", \"silhouette_type\", \"waist_type\",\n",
    "    \"sleeve_length_type\", \"heel_shape_type\", \"toecap_type\", \"woven_structure\",\n",
    "    \"knit_structure\", \"print_type\", \"archetype\", \"moment\", \"ocassion\",\n",
    "    \"phase_in\", \"phase_out\", \"life_cycle_length\",\n",
    "    \"num_stores\", \"num_sizes\", \"has_plus_size\", \"price\", \"year\"\n",
    "]\n",
    "\n",
    "static_train = (\n",
    "    train\n",
    "    .sort_values([\"ID\", \"num_week_iso\"])\n",
    "    .groupby(\"ID\")[static_cols]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "train_id = agg_train.merge(static_train, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4868249",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_test = (\n",
    "    test\n",
    "    .sort_values([\"ID\", \"num_week_iso\"])\n",
    "    .groupby(\"ID\")[static_cols]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087504a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_description(row):\n",
    "    parts = [\n",
    "        str(row.get(\"aggregated_family\", \"\")),\n",
    "        str(row.get(\"family\", \"\")),\n",
    "        str(row.get(\"category\", \"\")),\n",
    "        str(row.get(\"silhouette_type\", \"\")),\n",
    "        str(row.get(\"length_type\", \"\")),\n",
    "        str(row.get(\"waist_type\", \"\")),\n",
    "        str(row.get(\"sleeve_length_type\", \"\")),\n",
    "        str(row.get(\"fabric\", \"\")),\n",
    "        str(row.get(\"print_type\", \"\")),\n",
    "        str(row.get(\"color_name\", \"\")),\n",
    "        str(row.get(\"moment\", \"\")),\n",
    "        str(row.get(\"ocassion\", \"\")),\n",
    "        str(row.get(\"archetype\", \"\")),\n",
    "    ]\n",
    "    # Neteja i concatena\n",
    "    parts = [p for p in parts if p and p != \"nan\"]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "train_id[\"text_desc\"] = train_id.apply(build_text_description, axis=1)\n",
    "static_test[\"text_desc\"] = static_test.apply(build_text_description, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "train_text_emb = text_model.encode(\n",
    "    train_id[\"text_desc\"].tolist(),\n",
    "    batch_size=128,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "test_text_emb = text_model.encode(\n",
    "    static_test[\"text_desc\"].tolist(),\n",
    "    batch_size=128,\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_text = PCA(n_components=50, random_state=42)\n",
    "train_text_pca = pca_text.fit_transform(train_text_emb)\n",
    "test_text_pca = pca_text.transform(test_text_emb)\n",
    "\n",
    "# Els convertim a DataFrame per poder concatenar fàcilment\n",
    "text_cols = [f\"text_emb_{i}\" for i in range(train_text_pca.shape[1])]\n",
    "df_train_text = pd.DataFrame(train_text_pca, columns=text_cols, index=train_id.index)\n",
    "df_test_text = pd.DataFrame(test_text_pca, columns=text_cols, index=static_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fca34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_emb(x):\n",
    "    if isinstance(x, str):\n",
    "        return np.array(ast.literal_eval(x))\n",
    "    return np.array(x)\n",
    "\n",
    "img_emb_train = np.vstack(train_id[\"image_embedding\"].apply(parse_emb).values)\n",
    "img_emb_test = np.vstack(static_test[\"image_embedding\"].apply(parse_emb).values)\n",
    "\n",
    "# PCA opcional\n",
    "pca_img = PCA(n_components=50, random_state=42)\n",
    "train_img_pca = pca_img.fit_transform(img_emb_train)\n",
    "test_img_pca = pca_img.transform(img_emb_test)\n",
    "\n",
    "img_cols = [f\"img_emb_{i}\" for i in range(train_img_pca.shape[1])]\n",
    "df_train_img = pd.DataFrame(train_img_pca, columns=img_cols, index=train_id.index)\n",
    "df_test_img = pd.DataFrame(test_img_pca, columns=img_cols, index=static_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregat per categoria i any\n",
    "cat_year = (\n",
    "    train\n",
    "    .groupby([\"category\", \"year\"])\n",
    "    .agg(\n",
    "        cat_demand_total=(\"weekly_demand\", \"sum\"),\n",
    "        cat_price_mean=(\"price\", \"mean\"),\n",
    "        cat_num_products=(\"ID\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ordenem per categoria+any\n",
    "cat_year = cat_year.sort_values([\"category\", \"year\"])\n",
    "\n",
    "# Creixement interanual per categoria\n",
    "cat_year[\"cat_demand_total_prev\"] = (\n",
    "    cat_year.groupby(\"category\")[\"cat_demand_total\"].shift(1)\n",
    ")\n",
    "cat_year[\"cat_demand_growth\"] = (\n",
    "    cat_year[\"cat_demand_total\"] / cat_year[\"cat_demand_total_prev\"] - 1\n",
    ")\n",
    "\n",
    "# Omplim NaNs de primer any amb 0\n",
    "cat_year[\"cat_demand_growth\"] = cat_year[\"cat_demand_growth\"].fillna(0)\n",
    "\n",
    "# Podríem afegir més coses (rolling, slope, etc.) més endavant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd67202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train_id.merge(\n",
    "    cat_year[[\"category\", \"year\", \"cat_demand_total\", \"cat_demand_growth\", \"cat_price_mean\"]],\n",
    "    on=[\"category\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "static_test = static_test.merge(\n",
    "    cat_year[[\"category\", \"year\", \"cat_demand_total\", \"cat_demand_growth\", \"cat_price_mean\"]],\n",
    "    on=[\"category\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afegim els embeddings al DataFrame\n",
    "train_feat = pd.concat([train_id, df_train_text, df_train_img], axis=1)\n",
    "test_feat = pd.concat([static_test, df_test_text, df_test_img], axis=1)\n",
    "\n",
    "# Target\n",
    "y = train_feat[\"D_total\"].values\n",
    "\n",
    "# Treiem columnes que no volem com a features\n",
    "drop_cols = [\n",
    "    \"D_total\", \"sales_total\", \"first_week\", \"last_week\", \"life_cycle_real\",\n",
    "    \"text_desc\", \"image_embedding\",  # si encara hi són\n",
    "    \"weekly_sales\", \"weekly_demand\", \"Production\",  # per si han entrat d'alguna manera\n",
    "]\n",
    "\n",
    "for c in drop_cols:\n",
    "    if c in train_feat.columns:\n",
    "        train_feat = train_feat.drop(columns=c, errors=\"ignore\")\n",
    "    if c in test_feat.columns:\n",
    "        test_feat = test_feat.drop(columns=c, errors=\"ignore\")\n",
    "\n",
    "# Guardem ID per després fer la submission\n",
    "train_ids = train_feat[\"ID\"]\n",
    "test_ids = test_feat[\"ID\"]\n",
    "\n",
    "# No volem ID com a feature\n",
    "train_feat = train_feat.drop(columns=[\"ID\"])\n",
    "test_feat = test_feat.drop(columns=[\"ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931397de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train_feat.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "full = pd.concat([train_feat, test_feat], axis=0)\n",
    "\n",
    "full = pd.get_dummies(full, columns=cat_cols, dummy_na=True)\n",
    "\n",
    "train_X = full.iloc[:len(train_feat)].reset_index(drop=True)\n",
    "test_X = full.iloc[len(train_feat):].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c18c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = train_id[\"year\"].values  # mateix índex que train_feat abans de transformar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year = np.max(train_years)\n",
    "is_valid = train_years == last_year\n",
    "\n",
    "X_train = train_X[~is_valid]\n",
    "X_valid = train_X[is_valid]\n",
    "y_train = y[~is_valid]\n",
    "y_valid = y[is_valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cea083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb_quantile(X_tr, y_tr, X_va, y_va, alpha):\n",
    "    params = {\n",
    "        \"objective\": \"quantile\",\n",
    "        \"alpha\": alpha,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 63,\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"metric\": \"quantile\",\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_valid = lgb.Dataset(X_va, label=y_va)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        num_boost_round=2000,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "quantiles = [0.5, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "models = {}\n",
    "pred_valid = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    print(f\"Entrenando modelo para quantil {q}\")\n",
    "    m = train_lgb_quantile(X_train, y_train, X_valid, y_valid, alpha=q)\n",
    "    models[q] = m\n",
    "    pred_valid[q] = m.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_demand_real = y_valid  # D_total real de validación\n",
    "\n",
    "var_scores = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    prod_q = np.maximum(pred_valid[q], 1)  # evitamos 0 producción\n",
    "    full_price_sales_q = np.minimum(valid_demand_real, prod_q)\n",
    "    var_q = (full_price_sales_q / prod_q).mean()\n",
    "    var_scores[q] = var_q\n",
    "\n",
    "var_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358090ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_q = max(var_scores, key=var_scores.get)\n",
    "print(\"Mejor quantil:\", best_q, \"con VAR:\", var_scores[best_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[best_q]\n",
    "\n",
    "test_pred = best_model.predict(test_X)\n",
    "test_prod = np.maximum(test_pred, 1).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Production\": test_prod\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
